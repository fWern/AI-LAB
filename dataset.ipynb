{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8bce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94d8058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researchproblem</th>\n",
       "      <th>objective</th>\n",
       "      <th>method</th>\n",
       "      <th>result</th>\n",
       "      <th>conclusion</th>\n",
       "      <th>metatitle</th>\n",
       "      <th>metaauthor</th>\n",
       "      <th>researchfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A shortcoming of these existing assessment mod...</td>\n",
       "      <td>a graded maturity model for scholarly knowledg...</td>\n",
       "      <td>For developing and realizing the KGMM we follo...</td>\n",
       "      <td>Our model comprises 5 maturity stages with 20 ...</td>\n",
       "      <td>We demonstrate the implementation of our model...</td>\n",
       "      <td>KGMM - A Maturity Model for Scholarly Knowledg...</td>\n",
       "      <td>Hassan Hussein, Allard Oelen, Oliver Karras, S...</td>\n",
       "      <td>(\\uri{https://www.orkg.org/orkg/resource/R1121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crowd-sourcing for scientific knowledge graphs</td>\n",
       "      <td>workflow for authors of scientific documents t...</td>\n",
       "      <td>latex, luatex</td>\n",
       "      <td>score of 79 out of 100 on the System Usability...</td>\n",
       "      <td>SciKGTeX simplifies the process of manual sema...</td>\n",
       "      <td>\\title{SciKGTeX - A \\LaTeX{</td>\n",
       "      <td>\\uri{https://orcid.org/0000-0001-9778-8495, \\u...</td>\n",
       "      <td>(\\uri{https://orkg.org/resource/R278, Informat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     researchproblem  \\\n",
       "0  A shortcoming of these existing assessment mod...   \n",
       "1     crowd-sourcing for scientific knowledge graphs   \n",
       "\n",
       "                                           objective  \\\n",
       "0  a graded maturity model for scholarly knowledg...   \n",
       "1  workflow for authors of scientific documents t...   \n",
       "\n",
       "                                              method  \\\n",
       "0  For developing and realizing the KGMM we follo...   \n",
       "1                                      latex, luatex   \n",
       "\n",
       "                                              result  \\\n",
       "0  Our model comprises 5 maturity stages with 20 ...   \n",
       "1  score of 79 out of 100 on the System Usability...   \n",
       "\n",
       "                                          conclusion  \\\n",
       "0  We demonstrate the implementation of our model...   \n",
       "1  SciKGTeX simplifies the process of manual sema...   \n",
       "\n",
       "                                           metatitle  \\\n",
       "0  KGMM - A Maturity Model for Scholarly Knowledg...   \n",
       "1                        \\title{SciKGTeX - A \\LaTeX{   \n",
       "\n",
       "                                          metaauthor  \\\n",
       "0  Hassan Hussein, Allard Oelen, Oliver Karras, S...   \n",
       "1  \\uri{https://orcid.org/0000-0001-9778-8495, \\u...   \n",
       "\n",
       "                                       researchfield  \n",
       "0  (\\uri{https://www.orkg.org/orkg/resource/R1121...  \n",
       "1  (\\uri{https://orkg.org/resource/R278, Informat...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data(file_path):\n",
    "    files = os.listdir(file_path)\n",
    "    base_pattern = r'([^\\\\.]*\\\\{}{{[^}}]*}}[^\\\\.]*\\.)'\n",
    "    annotations = ['researchproblem', 'objective', 'method', 'result', 'conclusion']\n",
    "    dataset = []\n",
    "    for file_name in files:\n",
    "        if os.path.isfile(os.path.join(file_path, file_name)):\n",
    "            with open(os.path.join(file_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                for annotation in annotations:\n",
    "                    pattern = base_pattern.format(annotation)\n",
    "                    matches = re.findall(pattern, file_content)\n",
    "                    if matches:\n",
    "                        sentence = matches[0]\n",
    "                        cleaned_sentence = re.sub(r'^begin\\{abstract\\}\\\\?\\n?', '', sentence).strip()\n",
    "                        cleaned_sentence = re.sub(r'\\\\uri{[^{}]*}{([^{}]*)}', r'\\1', cleaned_sentence)\n",
    "                        cleaned_sentence = re.sub(r'\\\\' + annotation + r'{([^{}]*)}', r'\\1', cleaned_sentence).strip()\n",
    "                        \n",
    "                        uri_match = re.search(r'\\\\uri{([^}]*)}', sentence)\n",
    "                        if uri_match:\n",
    "                            uri = uri_match.group(1)\n",
    "                            dataset.append({'Annotation': annotation, 'Sentence': cleaned_sentence, 'URI': uri})\n",
    "                        else:\n",
    "                            dataset.append({'Annotation': annotation, 'Sentence': cleaned_sentence, 'URI': None})\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df\n",
    "\n",
    "df = extract_data('annotated_papers')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming get result from entity linking in format for orkg:\n",
    "# \"entities_orkg\": [\n",
    "#    {\n",
    "#        \"URI\": \"https://orkg.org/resource/R4322\",\n",
    "#        \"surface form\": \"machine learning\"\n",
    "#    }\n",
    "\n",
    "sample_text = \"this is a sample text for the example to add an entity, in this case machine learning, to an text.\"\n",
    "\n",
    "print(sample_text)\n",
    "\n",
    "def entity_linking(text, linking_results):\n",
    "    entities = linking_results[\"entities_orkg\"]\n",
    "    for entity in entities:\n",
    "        uri = entity['URI']\n",
    "        ent = entity['surface form']\n",
    "        text = text.replace(ent, f\"\\\\uri{{{uri}}}{{{ent}}}\")\n",
    "    return text\n",
    "\n",
    "linking_results = {\n",
    "    \"entities_orkg\": [\n",
    "        {\n",
    "            \"URI\": \"https://orkg.org/resource/R4322\",\n",
    "            \"surface form\": \"machine learning\"\n",
    "        },\n",
    "        {\n",
    "            \"URI\": \"https://orkg.org/resource/R214\",\n",
    "            \"surface form\": \"Chemical Engineering\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "entity_linking(sample_text, linking_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(sentence, keywords):\n",
    "    matched_keywords = sum(keyword in sentence.lower() for keyword in keywords)\n",
    "    probability = matched_keywords / len(keywords) if len(keywords) > 0 else 0.0\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(df):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    section_keywords = {\n",
    "        'researchproblem': [],\n",
    "        \"objective\": [],\n",
    "        \"method\": [],\n",
    "        \"result\": [],\n",
    "        'conclusion': []\n",
    "    }\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = row['Annotation']\n",
    "        sentence = row['Sentence']\n",
    "        \n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"] and token.text not in section_keywords[annotation]:\n",
    "                section_keywords[annotation].append(token.text.lower())\n",
    "    \n",
    "    return section_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(text):\n",
    "    threshold = 0.05\n",
    "    data = extract_data('annotated_papers')\n",
    "    \n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize variables to store labeled sections\n",
    "    labeled_sections = {\n",
    "        \"researchproblem\": [],\n",
    "        \"objective\": [],\n",
    "        \"method\": [],\n",
    "        \"result\": [],\n",
    "        \"conclusion\": []\n",
    "    }\n",
    "\n",
    "    # Define keywords that indicate different sections\n",
    "    annotation_keywords = extract_keywords(df)\n",
    "    \n",
    "    set_annotations = []\n",
    "\n",
    "    # Iterate through sentences in the processed text\n",
    "    for annotation, keywords in annotation_keywords.items():\n",
    "        probabilities = {}\n",
    "        for sent in doc.sents:\n",
    "            probability = calculate_probability(sent.text, keywords)\n",
    "            probabilities[probability] = sent.text\n",
    "        \n",
    "        print(annotation, probabilities)\n",
    "        best_prob = max(probabilities, key=probabilities.get)\n",
    "        best_sent = probabilities[best_prob]\n",
    "        \n",
    "        labeled_sections[annotation].append(best_sent)\n",
    "    \"\"\"for sent in doc.sents:\n",
    "        probabilities = {}\n",
    "        for section, keywords in section_keywords.items():\n",
    "            probability = calculate_probability(sent.text, keywords)\n",
    "            probabilities[section] = probability\n",
    "            \n",
    "        max_section = max(probabilities, key=probabilities.get)\n",
    "        max_probability = probabilities[max_section]\n",
    "        print(probabilities)\n",
    "        if max_section not in set_annotations and max_probability > threshold:\n",
    "            labeled_sections[max_section].append(sent.text)\n",
    "            set_annotations.append(max_section)\"\"\"\n",
    " \n",
    "    # Format the annotated text\n",
    "    annotated_text = \"\"\n",
    "    for section, sentences in labeled_sections.items():\n",
    "        annotated_text += f\"\\\\{section}{{{'. '.join(sentences)}}} \"\n",
    "\n",
    "    return annotated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aa771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = extract_data('annotated_papers')\n",
    "\n",
    "# Tokenize and vectorize sentences using TF-IDF\n",
    "#vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#sentence_vectors = vectorizer.fit_transform(df['Sentence'])\n",
    "\n",
    "def get_score(input_sentence, vectorizer, sentence_vectors):\n",
    "    input_vector = vectorizer.transform([input_sentence])\n",
    "    similarity_scores  = cosine_similarity(input_vector, sentence_vectors)\n",
    "    most_similar_index = similarity_scores.argmax()\n",
    "    return sentence_vectors[most_similar_index]\n",
    "\n",
    "# Example input sentence\n",
    "input_texts = [\"\"\"\n",
    "Feature models provide an effective way to organize and reuse requirements in a specific domain. A feature model \n",
    "consists of a feature tree and cross-tree constraints. Identifying features and then building a feature tree takes a \n",
    "lot of effort, and many semi-automated approaches have been proposed to help the situation. However, \n",
    "finding cross-tree constraints is often more challenging which still lacks the help of automation.\n",
    "In this paper, we propose an approach to mining cross-tree binary constraints in the construction of feature models. \n",
    "Binary constraints are the most basic kind of cross-tree constraints that involve exactly two features and can be further\n",
    "classified into two sub-types, i.e. requires and excludes. Given these two sub-types, a pair of any two features in a\n",
    "feature model falls into one of the following classes: no constraints between them, a requires between them,\n",
    "or an excludes between them. Therefore we perform a 3-class classification on feature pairs to mine binary\n",
    "constraints from features. We incorporate a support vector machine as the classifier and utilize a genetic algorithm to\n",
    "optimize it. We conduct a series of experiments on two feature models constructed by third parties, to \n",
    "evaluate the effectiveness of our approach under different conditions that might occur in practical use. \n",
    "Results show that we can mine binary constraints at a high recall (near 100\\% in most cases),\n",
    "which is important because finding a missing constraint is very costly in real, often large, feature models.\n",
    "\"\"\", \n",
    "               \n",
    "\"\"\"Modern requirements tracing tools employ information retrieval methods to automatically generate candidate links.\n",
    "Due to the inherent trade-off between recall and precision, such methods cannot achieve a high coverage without also \n",
    "retrieving a great number of false positives, causing a significant drop in result accuracy.\n",
    "In this paper, we propose an approach to improving the quality of candidate link generation for the requirements tracing\n",
    "process. We base our research on the cluster hypothesis which suggests that correct and incorrect links can be\n",
    "grouped in high-quality and low-quality clusters respectively.Result accuracy can thus be enhanced by identifying and\n",
    "filtering out low-quality clusters. We describe our approach by investigating three open-source datasets, and further\n",
    "evaluate our work through an industrial study. The results show that our approach outperforms a baseline pruning strategy\n",
    "and that improvements are still possible\"\"\",\n",
    "               \n",
    "\"\"\"Context-aware applications monitor changes in\n",
    "their operating environment and switch their behaviour to\n",
    "keep satisfying their requirements. Therefore, they must be\n",
    "equipped with the capability to detect variations in their\n",
    "operating context and to switch behaviour in response to\n",
    "such variations. However, specifying monitoring and\n",
    "switching in such applications can be difficult due to their\n",
    "dependence on varying contextual properties which need to\n",
    "be made explicit. In this paper, we present a problem-\n",
    "oriented approach to represent and reason about contextual\n",
    "variability and assess its impact on requirements; to elicit\n",
    "and specify concerns facing monitors and switchers, such as\n",
    "initialisation and interference; and to specify monitoring and\n",
    "switching behaviours that can detect changes and adapt in\n",
    "response. We illustrate our approach by applying it to a\n",
    "published case study.\"\"\",\n",
    "               \n",
    "\"\"\"Because of intense collaborative needs,\n",
    "requirements engineering is a challenge in global\n",
    "software development. How do distributed teams\n",
    "manage the development of requirements in\n",
    "environments that require significant cross-site\n",
    "collaboration and coordination? In this paper, we\n",
    "report research that used social network analysis to\n",
    "explore collaboration and awareness among team\n",
    "members during requirements management in an\n",
    "industrial distributed software team. Using the lens of\n",
    "a requirements-centred social network to group team\n",
    "members who work on a particular requirement, we\n",
    "collected data to characterize requirements-centric\n",
    "collaborations in a project, and to examine aspects of\n",
    "awareness of requirements changes within these\n",
    "networks. Our findings indicate organic patterns of\n",
    "collaboration involving considerable cross-site\n",
    "interaction, in which communication of changes was\n",
    "the most predominant reason for interaction. Although\n",
    "we did not find evidence that distance affects\n",
    "developers’ awareness of remote team members who\n",
    "work on the same requirements, distance affected how\n",
    "accessible the remote colleagues were. We discuss\n",
    "implications for knowledge sharing and coordination\n",
    "of work on a requirement in distributed teams, and\n",
    "propose directions for the design of collaboration tools\n",
    "that support awareness in distributed requirements\n",
    "management.\"\"\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "annotations = ['researchproblem', 'objective', 'method', 'result', 'conclusion']\n",
    "annotation_vectors = {}\n",
    "\n",
    "for annot in annotations:\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    sentence_vectors = vectorizer.fit_transform(df[df['Annotation'] == annot]['Sentence'])\n",
    "    annotation_vectors[annot] = (vectorizer, sentence_vectors)\n",
    "\n",
    "\n",
    "best_sentences = {\n",
    "    'researchproblem': None,\n",
    "    'objective': None,\n",
    "    'method': None,\n",
    "    'result': None,\n",
    "    'conclusion': None\n",
    "}\n",
    "\n",
    "for text in input_texts:\n",
    "    doc = nlp(text)\n",
    "    for annot in annotations:\n",
    "        vectorizer, sentence_vectors = annotation_vectors[annot]\n",
    "        best_sent = None\n",
    "        best_score = 0\n",
    "        for s in doc.sents:\n",
    "            score = get_score(s.text, vectorizer, sentence_vectors).mean()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sent = s.text\n",
    "\n",
    "        best_sentences[annot] = best_sent\n",
    "    \n",
    "    print(best_sentences)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4ba7d",
   "metadata": {},
   "source": [
    "### Fetch properties and resources from ORKG and create JSON to use with FALCON 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf667ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ssl\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def get_properties():\n",
    "    endpoint_url = \"https://orkg.org/triplestore\"\n",
    "\n",
    "    query = \"\"\"\n",
    "        PREFIX orkgp: <http://orkg.org/orkg/predicate/>\n",
    "        PREFIX orkgc: <http://orkg.org/orkg/class/>\n",
    "        PREFIX orkgr: <http://orkg.org/orkg/resource/>\n",
    "\n",
    "        SELECT ?property, ?label\n",
    "        WHERE {\n",
    "            ?property rdf:type orkgc:Predicate ;\n",
    "                rdfs:label ?label.\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "    \n",
    "    return results['results']['bindings']\n",
    "\n",
    "\n",
    "def get_entities(lower_bound, upper_bound):\n",
    "    endpoint_url = \"https://orkg.org/triplestore\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        PREFIX orkgp: <http://orkg.org/orkg/predicate/>\n",
    "        PREFIX orkgc: <http://orkg.org/orkg/class/>\n",
    "        PREFIX orkgr: <http://orkg.org/orkg/resource/>\n",
    "\n",
    "        SELECT ?entity, ?label\n",
    "        WHERE {{\n",
    "            ?entity rdfs:label ?label .\n",
    "            FILTER(STRSTARTS(STR(?entity), \"http://orkg.org/orkg/resource/R\") && \n",
    "            xsd:integer(STRAFTER(STR(?entity), \"http://orkg.org/orkg/resource/R\")) >= {lower_bound} &&\n",
    "            xsd:integer(STRAFTER(STR(?entity), \"http://orkg.org/orkg/resource/R\")) <= {upper_bound})\n",
    "        }}\n",
    "    \"\"\"\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "    \n",
    "    return results['results']['bindings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4614f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparql queries are limited to 100000 outputs, so have to get enities in batches\n",
    "ranges = [\n",
    "    (0, 100000), \n",
    "    (100001, 200000),\n",
    "    (200001, 300000), \n",
    "    (300001, 400000),\n",
    "    (400001, 500000), \n",
    "    (500001, 600000), \n",
    "    (600001, 700000)\n",
    "]\n",
    "\n",
    "entities = []\n",
    "for r in ranges:\n",
    "    entities.extend(get_entities(r[0], r[1]))\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(data, data_type):\n",
    "    if data_type == \"entity\":\n",
    "        file_path = \"orkgentity.json\"\n",
    "        index = \"orkgentityindex\"\n",
    "    elif data_type == \"property\":\n",
    "        file_path = \"orkgpropertyindex.json\"\n",
    "        index = \"orkgpropertyindex\"\n",
    "        \n",
    "    for entry in data:\n",
    "        new_json = {\n",
    "            \"_index\": index,\n",
    "            \"_type\": \"doc\",\n",
    "            \"_score\": 1,\n",
    "            \"_source\":{\n",
    "                \"uri\": \"<\"+entry[data_type]['value']+\">\",\n",
    "                \"label\": entry['label']['value']\n",
    "            }\n",
    "        }\n",
    "        with open(file_path, 'a') as json_file:\n",
    "            json.dump(new_json, json_file, indent=4)\n",
    "            json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json(properties, 'property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db12b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json(entities, 'entity')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
