{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8bce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08b1045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_path):\n",
    "    files = os.listdir(file_path)\n",
    "    dataset = []\n",
    "    for file_name in files:\n",
    "        if os.path.isfile(os.path.join(file_path, file_name)):\n",
    "            with open(os.path.join(file_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                pattern = r\"\\\\uri{(https?://[^\\s]+resource[^\\s]+)}{([^}]+)}\"\n",
    "                matches = re.findall(pattern, file_content)\n",
    "                for uri, entity in matches:\n",
    "                    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', file_content)\n",
    "                    for sentence in sentences:\n",
    "                        if uri in sentence and entity in sentence:\n",
    "                            sentence = re.sub(r\"\\\\uri{[^}]+}{[^}]+}\", entity, sentence)\n",
    "                            sentence = ' '.join(sentence.split())\n",
    "                            dataset.append({'Sentence': sentence, 'URI': uri, 'Entity': entity})\n",
    "                            break\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84a3bf51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>URI</th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\documentclass[sigconf, screen, authorversion]...</td>\n",
       "      <td>https://orkg.org/resource/R278</td>\n",
       "      <td>Information Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\researchproblem*{crowd-sourcing for scientifi...</td>\n",
       "      <td>https://orkg.org/resource/R606096</td>\n",
       "      <td>Crowd-sourcing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\researchproblem*{crowd-sourcing for scientifi...</td>\n",
       "      <td>https://orkg.org/resource/R257001</td>\n",
       "      <td>knowledge graphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\documentclass[sigconf, screen, authorversion]...</td>\n",
       "      <td>https://orkg.org/resource/R278</td>\n",
       "      <td>Information Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We incorporate a support vector machine as the...</td>\n",
       "      <td>https://orkg.org/resource/R3096</td>\n",
       "      <td>support vector machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We incorporate a genetic algorithm as the clas...</td>\n",
       "      <td>https://orkg.org/resource/R3072</td>\n",
       "      <td>genetic algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We conduct a series of experiments on two feat...</td>\n",
       "      <td>https://orkg.org/resource/R68581</td>\n",
       "      <td>effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\documentclass{article} \\usepackage[colorlinks...</td>\n",
       "      <td>https://orkg.org/resource/R591027</td>\n",
       "      <td>information retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Due to the inherent trade-off between recall a...</td>\n",
       "      <td>https://orkg.org/resource/R6557</td>\n",
       "      <td>recall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Due to the inherent trade-off between precisio...</td>\n",
       "      <td>https://orkg.org/resource/R6009</td>\n",
       "      <td>precision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>To evaluate its accuracy, we manually extracte...</td>\n",
       "      <td>https://orkg.org/resource/R6035</td>\n",
       "      <td>accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The \\conclusion{extraction precision and preci...</td>\n",
       "      <td>https://orkg.org/resource/R6009</td>\n",
       "      <td>precision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The \\conclusion{extraction recall and recall o...</td>\n",
       "      <td>https://orkg.org/resource/R6557</td>\n",
       "      <td>recall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\documentclass{article} \\usepackage[colorlinks...</td>\n",
       "      <td>https://orkg.org/resource/R104</td>\n",
       "      <td>Bioinformatics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\documentclass{article} \\usepackage[colorlinks...</td>\n",
       "      <td>https://orkg.org/resource/R212767</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In this paper, we propose \\objective{an automa...</td>\n",
       "      <td>https://orkg.org/resource/R162423</td>\n",
       "      <td>question-answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Our approach utilizes large-scale language mod...</td>\n",
       "      <td>https://orkg.org/resource/R146395</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Based on a large dataset of 15 JIRA repositori...</td>\n",
       "      <td>https://orkg.org/resource/R38183</td>\n",
       "      <td>machine learning models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>We found that a \\result{pure BERT model traine...</td>\n",
       "      <td>https://orkg.org/resource/R146395</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>We found that a \\result{pure macro F1-score mo...</td>\n",
       "      <td>https://orkg.org/resource/R162572</td>\n",
       "      <td>macro F1-score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>We found that a \\result{pure weighted F1-score...</td>\n",
       "      <td>https://orkg.org/resource/PWC_WEIGHTED_F1_C14024</td>\n",
       "      <td>weighted F1-score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\\documentclass{article} \\usepackage[colorlinks...</td>\n",
       "      <td>https://orkg.org/resource/R598165</td>\n",
       "      <td>requirements engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We also present an exploratory study in which ...</td>\n",
       "      <td>https://orkg.org/resource/R370996</td>\n",
       "      <td>exploratory study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We \\result{found that the Support Vector Machi...</td>\n",
       "      <td>https://orkg.org/resource/R3096</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>We \\result{found that the F1-score algorithm w...</td>\n",
       "      <td>https://orkg.org/resource/R75800</td>\n",
       "      <td>F1-score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Our approach uses a \\method{convolutional neur...</td>\n",
       "      <td>https://orkg.org/resource/R185379</td>\n",
       "      <td>convolutional neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In a 10-fold cross validation on a set of abou...</td>\n",
       "      <td>https://orkg.org/resource/R196406</td>\n",
       "      <td>10-fold cross validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(\\result{average precision 73\\%}).</td>\n",
       "      <td>https://orkg.org/resource/R286367</td>\n",
       "      <td>average precision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\\researchproblem{Previous research found that ...</td>\n",
       "      <td>https://orkg.org/resource/R205056</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>% LLNCS macro package for Springer Computer Sc...</td>\n",
       "      <td>https://www.orkg.org/orkg/resource/R112120</td>\n",
       "      <td>Digital Libraries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   \\documentclass[sigconf, screen, authorversion]...   \n",
       "1   \\researchproblem*{crowd-sourcing for scientifi...   \n",
       "2   \\researchproblem*{crowd-sourcing for scientifi...   \n",
       "3   \\documentclass[sigconf, screen, authorversion]...   \n",
       "4   We incorporate a support vector machine as the...   \n",
       "5   We incorporate a genetic algorithm as the clas...   \n",
       "6   We conduct a series of experiments on two feat...   \n",
       "7   \\documentclass{article} \\usepackage[colorlinks...   \n",
       "8   Due to the inherent trade-off between recall a...   \n",
       "9   Due to the inherent trade-off between precisio...   \n",
       "10  To evaluate its accuracy, we manually extracte...   \n",
       "11  The \\conclusion{extraction precision and preci...   \n",
       "12  The \\conclusion{extraction recall and recall o...   \n",
       "13  \\documentclass{article} \\usepackage[colorlinks...   \n",
       "14  \\documentclass{article} \\usepackage[colorlinks...   \n",
       "15  In this paper, we propose \\objective{an automa...   \n",
       "16  Our approach utilizes large-scale language mod...   \n",
       "17  Based on a large dataset of 15 JIRA repositori...   \n",
       "18  We found that a \\result{pure BERT model traine...   \n",
       "19  We found that a \\result{pure macro F1-score mo...   \n",
       "20  We found that a \\result{pure weighted F1-score...   \n",
       "21  \\documentclass{article} \\usepackage[colorlinks...   \n",
       "22  We also present an exploratory study in which ...   \n",
       "23  We \\result{found that the Support Vector Machi...   \n",
       "24  We \\result{found that the F1-score algorithm w...   \n",
       "25  Our approach uses a \\method{convolutional neur...   \n",
       "26  In a 10-fold cross validation on a set of abou...   \n",
       "27                 (\\result{average precision 73\\%}).   \n",
       "28  \\researchproblem{Previous research found that ...   \n",
       "29  % LLNCS macro package for Springer Computer Sc...   \n",
       "\n",
       "                                                 URI  \\\n",
       "0                     https://orkg.org/resource/R278   \n",
       "1                  https://orkg.org/resource/R606096   \n",
       "2                  https://orkg.org/resource/R257001   \n",
       "3                     https://orkg.org/resource/R278   \n",
       "4                    https://orkg.org/resource/R3096   \n",
       "5                    https://orkg.org/resource/R3072   \n",
       "6                   https://orkg.org/resource/R68581   \n",
       "7                  https://orkg.org/resource/R591027   \n",
       "8                    https://orkg.org/resource/R6557   \n",
       "9                    https://orkg.org/resource/R6009   \n",
       "10                   https://orkg.org/resource/R6035   \n",
       "11                   https://orkg.org/resource/R6009   \n",
       "12                   https://orkg.org/resource/R6557   \n",
       "13                    https://orkg.org/resource/R104   \n",
       "14                 https://orkg.org/resource/R212767   \n",
       "15                 https://orkg.org/resource/R162423   \n",
       "16                 https://orkg.org/resource/R146395   \n",
       "17                  https://orkg.org/resource/R38183   \n",
       "18                 https://orkg.org/resource/R146395   \n",
       "19                 https://orkg.org/resource/R162572   \n",
       "20  https://orkg.org/resource/PWC_WEIGHTED_F1_C14024   \n",
       "21                 https://orkg.org/resource/R598165   \n",
       "22                 https://orkg.org/resource/R370996   \n",
       "23                   https://orkg.org/resource/R3096   \n",
       "24                  https://orkg.org/resource/R75800   \n",
       "25                 https://orkg.org/resource/R185379   \n",
       "26                 https://orkg.org/resource/R196406   \n",
       "27                 https://orkg.org/resource/R286367   \n",
       "28                 https://orkg.org/resource/R205056   \n",
       "29        https://www.orkg.org/orkg/resource/R112120   \n",
       "\n",
       "                          Entity  \n",
       "0            Information Science  \n",
       "1                 Crowd-sourcing  \n",
       "2               knowledge graphs  \n",
       "3            Information Science  \n",
       "4         support vector machine  \n",
       "5              genetic algorithm  \n",
       "6                  effectiveness  \n",
       "7          information retrieval  \n",
       "8                         recall  \n",
       "9                      precision  \n",
       "10                      accuracy  \n",
       "11                     precision  \n",
       "12                        recall  \n",
       "13                Bioinformatics  \n",
       "14                      COVID-19  \n",
       "15            question-answering  \n",
       "16                          BERT  \n",
       "17       machine learning models  \n",
       "18                          BERT  \n",
       "19                macro F1-score  \n",
       "20             weighted F1-score  \n",
       "21      requirements engineering  \n",
       "22             exploratory study  \n",
       "23        Support Vector Machine  \n",
       "24                      F1-score  \n",
       "25  convolutional neural network  \n",
       "26      10-fold cross validation  \n",
       "27             average precision  \n",
       "28                        tweets  \n",
       "29             Digital Libraries  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_data('annotated_papers')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed9c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample text for the example to add an entity, in this case machine learning, to an text.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a \\\\uri{http://orkg.org/orkg/resource/R4647}{sample text for the example} to add an \\\\uri{http://orkg.org/orkg/resource/R4322}{entity, in this case machine learning, to an text}.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entity_linking(text, linking_results):\n",
    "    for entity in linking_results:\n",
    "        uri = entity[0]\n",
    "        ent = entity[1].lower()\n",
    "        text = text.replace(ent, f\"\\\\uri{{{uri}}}{{{ent}}}\")\n",
    "    return text\n",
    "\n",
    "sample_text = \"this is a sample text for the example to add an entity, in this case machine learning, to an text.\"\n",
    "print(sample_text)\n",
    "\n",
    "# results from falcon\n",
    "linking_results = [['http://orkg.org/orkg/resource/R4322', 'entity, in this case machine learning, to an text'], \n",
    "                   ['http://orkg.org/orkg/resource/R4647', 'sample text for the example']]\n",
    "\n",
    "entity_linking(sample_text, linking_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4ba7d",
   "metadata": {},
   "source": [
    "### Fetch properties and resources from ORKG and create JSON to use with FALCON 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf667ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ssl\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def get_properties():\n",
    "    endpoint_url = \"https://orkg.org/triplestore\"\n",
    "\n",
    "    query = \"\"\"\n",
    "        PREFIX orkgp: <http://orkg.org/orkg/predicate/>\n",
    "        PREFIX orkgc: <http://orkg.org/orkg/class/>\n",
    "        PREFIX orkgr: <http://orkg.org/orkg/resource/>\n",
    "\n",
    "        SELECT ?property, ?label\n",
    "        WHERE {\n",
    "            ?property rdf:type orkgc:Predicate ;\n",
    "                rdfs:label ?label.\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "    \n",
    "    return results['results']['bindings']\n",
    "\n",
    "\n",
    "def get_entities(lower_bound, upper_bound):\n",
    "    endpoint_url = \"https://orkg.org/triplestore\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        PREFIX orkgp: <http://orkg.org/orkg/predicate/>\n",
    "        PREFIX orkgc: <http://orkg.org/orkg/class/>\n",
    "        PREFIX orkgr: <http://orkg.org/orkg/resource/>\n",
    "\n",
    "        SELECT ?entity, ?label\n",
    "        WHERE {{\n",
    "            ?entity rdfs:label ?label .\n",
    "            FILTER(STRSTARTS(STR(?entity), \"http://orkg.org/orkg/resource/R\") && \n",
    "            xsd:integer(STRAFTER(STR(?entity), \"http://orkg.org/orkg/resource/R\")) >= {lower_bound} &&\n",
    "            xsd:integer(STRAFTER(STR(?entity), \"http://orkg.org/orkg/resource/R\")) <= {upper_bound})\n",
    "        }}\n",
    "    \"\"\"\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "    \n",
    "    return results['results']['bindings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4614f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparql queries are limited to 100000 outputs, so have to get enities in batches\n",
    "ranges = [\n",
    "    (0, 100000), \n",
    "    (100001, 200000),\n",
    "    (200001, 300000), \n",
    "    (300001, 400000),\n",
    "    (400001, 500000), \n",
    "    (500001, 600000), \n",
    "    (600001, 700000)\n",
    "]\n",
    "\n",
    "entities = []\n",
    "for r in ranges:\n",
    "    entities.extend(get_entities(r[0], r[1]))\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(data, data_type):\n",
    "    if data_type == \"entity\":\n",
    "        file_path = \"orkgentity.json\"\n",
    "        index = \"orkgentityindex\"\n",
    "    elif data_type == \"property\":\n",
    "        file_path = \"orkgpropertyindex.json\"\n",
    "        index = \"orkgpropertyindex\"\n",
    "        \n",
    "    for entry in data:\n",
    "        new_json = {\n",
    "            \"_index\": index,\n",
    "            \"_type\": \"doc\",\n",
    "            \"_score\": 1,\n",
    "            \"_source\":{\n",
    "                \"uri\": \"<\"+entry[data_type]['value']+\">\",\n",
    "                \"label\": entry['label']['value']\n",
    "            }\n",
    "        }\n",
    "        with open(file_path, 'a') as json_file:\n",
    "            json.dump(new_json, json_file, indent=4)\n",
    "            json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json(properties, 'property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db12b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json(entities, 'entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5e694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(sentence, keywords):\n",
    "    matched_keywords = sum(keyword in sentence.lower() for keyword in keywords)\n",
    "    probability = matched_keywords / len(keywords) if len(keywords) > 0 else 0.0\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(df):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    section_keywords = {\n",
    "        'researchproblem': [],\n",
    "        \"objective\": [],\n",
    "        \"method\": [],\n",
    "        \"result\": [],\n",
    "        'conclusion': []\n",
    "    }\n",
    "    for index, row in df.iterrows():\n",
    "        annotation = row['Annotation']\n",
    "        sentence = row['Sentence']\n",
    "        \n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"] and token.text not in section_keywords[annotation]:\n",
    "                section_keywords[annotation].append(token.text.lower())\n",
    "    \n",
    "    return section_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(text):\n",
    "    threshold = 0.05\n",
    "    data = extract_data('annotated_papers')\n",
    "    \n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize variables to store labeled sections\n",
    "    labeled_sections = {\n",
    "        \"researchproblem\": [],\n",
    "        \"objective\": [],\n",
    "        \"method\": [],\n",
    "        \"result\": [],\n",
    "        \"conclusion\": []\n",
    "    }\n",
    "\n",
    "    # Define keywords that indicate different sections\n",
    "    annotation_keywords = extract_keywords(df)\n",
    "    \n",
    "    set_annotations = []\n",
    "\n",
    "    # Iterate through sentences in the processed text\n",
    "    for annotation, keywords in annotation_keywords.items():\n",
    "        probabilities = {}\n",
    "        for sent in doc.sents:\n",
    "            probability = calculate_probability(sent.text, keywords)\n",
    "            probabilities[probability] = sent.text\n",
    "        \n",
    "        print(annotation, probabilities)\n",
    "        best_prob = max(probabilities, key=probabilities.get)\n",
    "        best_sent = probabilities[best_prob]\n",
    "        \n",
    "        labeled_sections[annotation].append(best_sent)\n",
    "    \"\"\"for sent in doc.sents:\n",
    "        probabilities = {}\n",
    "        for section, keywords in section_keywords.items():\n",
    "            probability = calculate_probability(sent.text, keywords)\n",
    "            probabilities[section] = probability\n",
    "            \n",
    "        max_section = max(probabilities, key=probabilities.get)\n",
    "        max_probability = probabilities[max_section]\n",
    "        print(probabilities)\n",
    "        if max_section not in set_annotations and max_probability > threshold:\n",
    "            labeled_sections[max_section].append(sent.text)\n",
    "            set_annotations.append(max_section)\"\"\"\n",
    " \n",
    "    # Format the annotated text\n",
    "    annotated_text = \"\"\n",
    "    for section, sentences in labeled_sections.items():\n",
    "        annotated_text += f\"\\\\{section}{{{'. '.join(sentences)}}} \"\n",
    "\n",
    "    return annotated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aa771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = extract_data('annotated_papers')\n",
    "\n",
    "# Tokenize and vectorize sentences using TF-IDF\n",
    "#vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#sentence_vectors = vectorizer.fit_transform(df['Sentence'])\n",
    "\n",
    "def get_score(input_sentence, vectorizer, sentence_vectors):\n",
    "    input_vector = vectorizer.transform([input_sentence])\n",
    "    similarity_scores  = cosine_similarity(input_vector, sentence_vectors)\n",
    "    most_similar_index = similarity_scores.argmax()\n",
    "    return sentence_vectors[most_similar_index]\n",
    "\n",
    "# Example input sentence\n",
    "input_texts = [\"\"\"\n",
    "Feature models provide an effective way to organize and reuse requirements in a specific domain. A feature model \n",
    "consists of a feature tree and cross-tree constraints. Identifying features and then building a feature tree takes a \n",
    "lot of effort, and many semi-automated approaches have been proposed to help the situation. However, \n",
    "finding cross-tree constraints is often more challenging which still lacks the help of automation.\n",
    "In this paper, we propose an approach to mining cross-tree binary constraints in the construction of feature models. \n",
    "Binary constraints are the most basic kind of cross-tree constraints that involve exactly two features and can be further\n",
    "classified into two sub-types, i.e. requires and excludes. Given these two sub-types, a pair of any two features in a\n",
    "feature model falls into one of the following classes: no constraints between them, a requires between them,\n",
    "or an excludes between them. Therefore we perform a 3-class classification on feature pairs to mine binary\n",
    "constraints from features. We incorporate a support vector machine as the classifier and utilize a genetic algorithm to\n",
    "optimize it. We conduct a series of experiments on two feature models constructed by third parties, to \n",
    "evaluate the effectiveness of our approach under different conditions that might occur in practical use. \n",
    "Results show that we can mine binary constraints at a high recall (near 100\\% in most cases),\n",
    "which is important because finding a missing constraint is very costly in real, often large, feature models.\n",
    "\"\"\", \n",
    "               \n",
    "\"\"\"Modern requirements tracing tools employ information retrieval methods to automatically generate candidate links.\n",
    "Due to the inherent trade-off between recall and precision, such methods cannot achieve a high coverage without also \n",
    "retrieving a great number of false positives, causing a significant drop in result accuracy.\n",
    "In this paper, we propose an approach to improving the quality of candidate link generation for the requirements tracing\n",
    "process. We base our research on the cluster hypothesis which suggests that correct and incorrect links can be\n",
    "grouped in high-quality and low-quality clusters respectively.Result accuracy can thus be enhanced by identifying and\n",
    "filtering out low-quality clusters. We describe our approach by investigating three open-source datasets, and further\n",
    "evaluate our work through an industrial study. The results show that our approach outperforms a baseline pruning strategy\n",
    "and that improvements are still possible\"\"\",\n",
    "               \n",
    "\"\"\"Context-aware applications monitor changes in\n",
    "their operating environment and switch their behaviour to\n",
    "keep satisfying their requirements. Therefore, they must be\n",
    "equipped with the capability to detect variations in their\n",
    "operating context and to switch behaviour in response to\n",
    "such variations. However, specifying monitoring and\n",
    "switching in such applications can be difficult due to their\n",
    "dependence on varying contextual properties which need to\n",
    "be made explicit. In this paper, we present a problem-\n",
    "oriented approach to represent and reason about contextual\n",
    "variability and assess its impact on requirements; to elicit\n",
    "and specify concerns facing monitors and switchers, such as\n",
    "initialisation and interference; and to specify monitoring and\n",
    "switching behaviours that can detect changes and adapt in\n",
    "response. We illustrate our approach by applying it to a\n",
    "published case study.\"\"\",\n",
    "               \n",
    "\"\"\"Because of intense collaborative needs,\n",
    "requirements engineering is a challenge in global\n",
    "software development. How do distributed teams\n",
    "manage the development of requirements in\n",
    "environments that require significant cross-site\n",
    "collaboration and coordination? In this paper, we\n",
    "report research that used social network analysis to\n",
    "explore collaboration and awareness among team\n",
    "members during requirements management in an\n",
    "industrial distributed software team. Using the lens of\n",
    "a requirements-centred social network to group team\n",
    "members who work on a particular requirement, we\n",
    "collected data to characterize requirements-centric\n",
    "collaborations in a project, and to examine aspects of\n",
    "awareness of requirements changes within these\n",
    "networks. Our findings indicate organic patterns of\n",
    "collaboration involving considerable cross-site\n",
    "interaction, in which communication of changes was\n",
    "the most predominant reason for interaction. Although\n",
    "we did not find evidence that distance affects\n",
    "developers’ awareness of remote team members who\n",
    "work on the same requirements, distance affected how\n",
    "accessible the remote colleagues were. We discuss\n",
    "implications for knowledge sharing and coordination\n",
    "of work on a requirement in distributed teams, and\n",
    "propose directions for the design of collaboration tools\n",
    "that support awareness in distributed requirements\n",
    "management.\"\"\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "annotations = ['researchproblem', 'objective', 'method', 'result', 'conclusion']\n",
    "annotation_vectors = {}\n",
    "\n",
    "for annot in annotations:\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    sentence_vectors = vectorizer.fit_transform(df[df['Annotation'] == annot]['Sentence'])\n",
    "    annotation_vectors[annot] = (vectorizer, sentence_vectors)\n",
    "\n",
    "\n",
    "best_sentences = {\n",
    "    'researchproblem': None,\n",
    "    'objective': None,\n",
    "    'method': None,\n",
    "    'result': None,\n",
    "    'conclusion': None\n",
    "}\n",
    "\n",
    "for text in input_texts:\n",
    "    doc = nlp(text)\n",
    "    for annot in annotations:\n",
    "        vectorizer, sentence_vectors = annotation_vectors[annot]\n",
    "        best_sent = None\n",
    "        best_score = 0\n",
    "        for s in doc.sents:\n",
    "            score = get_score(s.text, vectorizer, sentence_vectors).mean()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sent = s.text\n",
    "\n",
    "        best_sentences[annot] = best_sent\n",
    "    \n",
    "    print(best_sentences)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94d8058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>URI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>result</td>\n",
       "      <td>result{score of 79 out of 100 on the System Us...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we present a workflow for autho...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>result</td>\n",
       "      <td>Our user evaluation shows that SciKGTeX is eas...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>SciKGTeX simplifies the process of manual sema...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>However, finding cross-tree constraints is oft...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we propose an approach to minin...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>result</td>\n",
       "      <td>Results show that we can mine binary constrain...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>\\researchproblem{Modern requirements tracing t...</td>\n",
       "      <td>https://orkg.org/resource/R591027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we propose an approach to impro...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>method</td>\n",
       "      <td>We base our research on the cluster hypothesis...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>result</td>\n",
       "      <td>The results show that our approach outperforms...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>However, extracting and matching the features ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we propose SAFE, a novel unifor...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>method</td>\n",
       "      <td>We manually build 18 part-of-speech patterns a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>result</td>\n",
       "      <td>For well-maintained app pages such as for Goog...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>\\researchproblem{With high inflammatory states...</td>\n",
       "      <td>https://orkg.org/resource/R212767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>objective</td>\n",
       "      <td>The aim of this study was to figure out the ph...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>method</td>\n",
       "      <td>We employed computer-aided methods to uncover ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>result</td>\n",
       "      <td>Stress-responsive, membrane receptor, and indu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>Consequently, advanced solutions are required ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>Extracting compliance requirements from regula...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we propose \\objective{an automa...</td>\n",
       "      <td>https://orkg.org/resource/R162423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>method</td>\n",
       "      <td>We evaluate our approach on 107 question-answe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>result</td>\n",
       "      <td>Our empirical results show that, in ≈94\\% of t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>Stakeholders in software projects use issue tr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>objective</td>\n",
       "      <td>Based on a large dataset of 15 JIRA repositori...</td>\n",
       "      <td>https://orkg.org/resource/R38183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>We found that Relate-links often get confused ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>Recent literature on mining software-related d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we introduce reddit as a new po...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>method</td>\n",
       "      <td>Finally, we investigated the potential of auto...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>Our results show that reddit posts provide use...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>An important task in requirements engineering ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this paper, we propose an automatic approac...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>method</td>\n",
       "      <td>Our approach uses a \\method{convolutional neur...</td>\n",
       "      <td>https://orkg.org/resource/R185379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>result</td>\n",
       "      <td>org/resource/R196406}{10-fold cross validation...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>The results show that our approach might help ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>However, it is often challenging to determine ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>objective</td>\n",
       "      <td>To this end, in this paper, we propose a novel...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>method</td>\n",
       "      <td>Second, we train a machine classifier with the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>We evaluate the proposed approach on the datas...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>\\researchproblem{Previous research found that ...</td>\n",
       "      <td>https://orkg.org/resource/R205056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this work we present ALERTme, an approach t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>method</td>\n",
       "      <td>We apply machine learning techniques for autom...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>Our results show that ALERTme is an effective ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>researchproblem</td>\n",
       "      <td>A shortcoming of these existing assessment mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>objective</td>\n",
       "      <td>In this work, we propose a graded maturity mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>result</td>\n",
       "      <td>Our model comprises 5 maturity stages with 20 ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>We demonstrate the implementation of our model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Annotation                                           Sentence  \\\n",
       "0            result  result{score of 79 out of 100 on the System Us...   \n",
       "1         objective  In this paper, we present a workflow for autho...   \n",
       "2            result  Our user evaluation shows that SciKGTeX is eas...   \n",
       "3        conclusion  SciKGTeX simplifies the process of manual sema...   \n",
       "4   researchproblem  However, finding cross-tree constraints is oft...   \n",
       "5         objective  In this paper, we propose an approach to minin...   \n",
       "6            result  Results show that we can mine binary constrain...   \n",
       "7   researchproblem  \\researchproblem{Modern requirements tracing t...   \n",
       "8         objective  In this paper, we propose an approach to impro...   \n",
       "9            method  We base our research on the cluster hypothesis...   \n",
       "10           result  The results show that our approach outperforms...   \n",
       "11  researchproblem  However, extracting and matching the features ...   \n",
       "12        objective  In this paper, we propose SAFE, a novel unifor...   \n",
       "13           method  We manually build 18 part-of-speech patterns a...   \n",
       "14           result  For well-maintained app pages such as for Goog...   \n",
       "15  researchproblem  \\researchproblem{With high inflammatory states...   \n",
       "16        objective  The aim of this study was to figure out the ph...   \n",
       "17           method  We employed computer-aided methods to uncover ...   \n",
       "18           result  Stress-responsive, membrane receptor, and indu...   \n",
       "19       conclusion  Consequently, advanced solutions are required ...   \n",
       "20  researchproblem  Extracting compliance requirements from regula...   \n",
       "21        objective  In this paper, we propose \\objective{an automa...   \n",
       "22           method  We evaluate our approach on 107 question-answe...   \n",
       "23           result  Our empirical results show that, in ≈94\\% of t...   \n",
       "24  researchproblem  Stakeholders in software projects use issue tr...   \n",
       "25        objective  Based on a large dataset of 15 JIRA repositori...   \n",
       "26       conclusion  We found that Relate-links often get confused ...   \n",
       "27  researchproblem  Recent literature on mining software-related d...   \n",
       "28        objective  In this paper, we introduce reddit as a new po...   \n",
       "29           method  Finally, we investigated the potential of auto...   \n",
       "30       conclusion  Our results show that reddit posts provide use...   \n",
       "31  researchproblem  An important task in requirements engineering ...   \n",
       "32        objective  In this paper, we propose an automatic approac...   \n",
       "33           method  Our approach uses a \\method{convolutional neur...   \n",
       "34           result  org/resource/R196406}{10-fold cross validation...   \n",
       "35       conclusion  The results show that our approach might help ...   \n",
       "36  researchproblem  However, it is often challenging to determine ...   \n",
       "37        objective  To this end, in this paper, we propose a novel...   \n",
       "38           method  Second, we train a machine classifier with the...   \n",
       "39       conclusion  We evaluate the proposed approach on the datas...   \n",
       "40  researchproblem  \\researchproblem{Previous research found that ...   \n",
       "41        objective  In this work we present ALERTme, an approach t...   \n",
       "42           method  We apply machine learning techniques for autom...   \n",
       "43       conclusion  Our results show that ALERTme is an effective ...   \n",
       "44  researchproblem  A shortcoming of these existing assessment mod...   \n",
       "45        objective  In this work, we propose a graded maturity mod...   \n",
       "46           result  Our model comprises 5 maturity stages with 20 ...   \n",
       "47       conclusion  We demonstrate the implementation of our model...   \n",
       "\n",
       "                                  URI  \n",
       "0                                None  \n",
       "1                                None  \n",
       "2                                None  \n",
       "3                                None  \n",
       "4                                None  \n",
       "5                                None  \n",
       "6                                None  \n",
       "7   https://orkg.org/resource/R591027  \n",
       "8                                None  \n",
       "9                                None  \n",
       "10                               None  \n",
       "11                               None  \n",
       "12                               None  \n",
       "13                               None  \n",
       "14                               None  \n",
       "15  https://orkg.org/resource/R212767  \n",
       "16                               None  \n",
       "17                               None  \n",
       "18                               None  \n",
       "19                               None  \n",
       "20                               None  \n",
       "21  https://orkg.org/resource/R162423  \n",
       "22                               None  \n",
       "23                               None  \n",
       "24                               None  \n",
       "25   https://orkg.org/resource/R38183  \n",
       "26                               None  \n",
       "27                               None  \n",
       "28                               None  \n",
       "29                               None  \n",
       "30                               None  \n",
       "31                               None  \n",
       "32                               None  \n",
       "33  https://orkg.org/resource/R185379  \n",
       "34                               None  \n",
       "35                               None  \n",
       "36                               None  \n",
       "37                               None  \n",
       "38                               None  \n",
       "39                               None  \n",
       "40  https://orkg.org/resource/R205056  \n",
       "41                               None  \n",
       "42                               None  \n",
       "43                               None  \n",
       "44                               None  \n",
       "45                               None  \n",
       "46                               None  \n",
       "47                               None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data(file_path):\n",
    "    files = os.listdir(file_path)\n",
    "    base_pattern = r'([^\\\\.]*\\\\{}{{[^}}]*}}[^\\\\.]*\\.)'\n",
    "    annotations = ['researchproblem', 'objective', 'method', 'result', 'conclusion']\n",
    "    dataset = []\n",
    "    for file_name in files:\n",
    "        if os.path.isfile(os.path.join(file_path, file_name)):\n",
    "            with open(os.path.join(file_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                for annotation in annotations:\n",
    "                    pattern = base_pattern.format(annotation)\n",
    "                    matches = re.findall(pattern, file_content)\n",
    "                    if matches:\n",
    "                        sentence = matches[0]\n",
    "                        cleaned_sentence = re.sub(r'^begin\\{abstract\\}\\\\?\\n?', '', sentence).strip()\n",
    "                        cleaned_sentence = re.sub(r'\\\\uri{[^{}]*}{([^{}]*)}', r'\\1', cleaned_sentence)\n",
    "                        cleaned_sentence = re.sub(r'\\\\' + annotation + r'{([^{}]*)}', r'\\1', cleaned_sentence).strip()\n",
    "                        \n",
    "                        uri_match = re.search(r'\\\\uri{([^}]*)}', sentence)\n",
    "                        if uri_match:\n",
    "                            uri = uri_match.group(1)\n",
    "                            dataset.append({'Annotation': annotation, 'Sentence': cleaned_sentence, 'URI': uri})\n",
    "                        else:\n",
    "                            dataset.append({'Annotation': annotation, 'Sentence': cleaned_sentence, 'URI': None})\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df\n",
    "\n",
    "df = extract_data('annotated_papers')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
